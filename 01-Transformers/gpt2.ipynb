{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00a6971f-17b6-43fd-95b5-a7dd70d77ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f333e102-7059-4de8-be4e-837e338df317",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35f3d1f8-1c52-4cb3-b61e-29a4591dd984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[37534,  6197,   516,    11,  8335,   314,  1101,   781,   397,  3900,\n",
       "          8992,     0]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Preposterous, prepare I'm flabbergasted!\"\n",
    "input_ids = tokenizer(sentence, return_tensors='pt').input_ids\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "862bb894-2144-4766-8ca4-6968c6bb5f70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Prep', 'oster', ' prepare')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# words --> tokens --> Unique ID --> vector embed\n",
    "tokenizer.decode(37534), tokenizer.decode(6197), tokenizer.decode(8335) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e5b8189d-daf7-4cf5-a2f5-b0741c4e1cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prep\n",
      "oster\n",
      "ous\n",
      ",\n",
      " prepare\n",
      " I\n",
      "'m\n",
      " fl\n",
      "ab\n",
      "berg\n",
      "asted\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "for token_id in input_ids[0]:\n",
    "    print(tokenizer.decode(token_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2ca5d429-cbe0-4942-a6e6-9a97ca137f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"I skipped across the\"\n",
    "input_ids = tokenizer(sentence, return_tensors='pt').input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9330e392-41f4-4f08-a7a2-2dde3eb982ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      " skipped\n",
      " across\n",
      " the\n"
     ]
    }
   ],
   "source": [
    "for token_id in input_ids[0]:\n",
    "    print(tokenizer.decode(token_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "76cc18d9-7e7a-4e45-8dde-4bbc8107f337",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2 = AutoModelForCausalLM.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "15e2ec04-7851-46da-b777-14b66a641c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = gpt2(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "65dea9cc-0116-4448-9838-4b18754ecbe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 50257])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ -39.3084,  -39.0100,  -41.8374,  ...,  -46.9337,  -44.9074,\n",
       "           -39.5149],\n",
       "         [ -95.4915,  -95.7003, -100.9737,  ..., -105.3830, -105.3877,\n",
       "           -97.9453],\n",
       "         [ -77.7412,  -78.2803,  -81.9967,  ...,  -85.0172,  -86.6475,\n",
       "           -80.5391],\n",
       "         [ -89.3046,  -87.9105,  -89.7545,  ...,  -90.2860,  -92.4808,\n",
       "           -88.3736]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(outputs.logits.shape)\n",
    "outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6563ac0c-105b-4429-8232-587e5ecbea5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-89.3046, -87.9105, -89.7545,  ..., -90.2860, -92.4808, -88.3736],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_logits = gpt2(input_ids).logits[0,-1]\n",
    "final_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "64420c43-0dcb-4647-9e38-f2f66e30a19a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4675)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_logits.argmax() # Token ID <--> Index location logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "199ec2a8-1823-4781-961b-ad552c5f2c6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' street'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(final_logits.argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1ac53631-53c9-4956-a9bd-bd74106bd55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "top10_logits = torch.topk(final_logits, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "df885e51-7cc6-48e5-bf6a-7d00b6e1824c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " street\n",
      " line\n",
      " room\n",
      " country\n",
      " river\n",
      " border\n",
      " pond\n",
      " bridge\n",
      " floor\n",
      " road\n"
     ]
    }
   ],
   "source": [
    "for index in top10_logits.indices:\n",
    "    print(tokenizer.decode(index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5a22eb98-b8f7-4a01-9061-d5934053558b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6.1705e-07, 2.4875e-06, 3.9347e-07,  ..., 2.3124e-07, 2.5757e-08,\n",
       "        1.5653e-06], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_logits.softmax(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "29d6dd86-8928-4c63-b21a-e12d805af8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "top10 = torch.topk(final_logits.softmax(dim=0), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1d032751-3a74-4f0c-a7f4-c0d833181f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " street -- 3.8%\n",
      " line -- 3.2%\n",
      " room -- 2.1%\n",
      " country -- 1.8%\n",
      " river -- 1.6%\n",
      " border -- 1.5%\n",
      " pond -- 1.5%\n",
      " bridge -- 1.3%\n",
      " floor -- 1.3%\n",
      " road -- 1.2%\n"
     ]
    }
   ],
   "source": [
    "for value, index in zip(top10.values, top10.indices):\n",
    "    print(f\"{tokenizer.decode(index)} -- {value.item():.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b816d39-0def-4d9e-a590-5cc002269e94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
