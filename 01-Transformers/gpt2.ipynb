{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00a6971f-17b6-43fd-95b5-a7dd70d77ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f333e102-7059-4de8-be4e-837e338df317",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35f3d1f8-1c52-4cb3-b61e-29a4591dd984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[37534,  6197,   516,    11,  8335,   314,  1101,   781,   397,  3900,\n",
       "          8992,     0]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Preposterous, prepare I'm flabbergasted!\"\n",
    "input_ids = tokenizer(sentence, return_tensors='pt').input_ids\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "862bb894-2144-4766-8ca4-6968c6bb5f70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Prep', 'oster', ' prepare')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# words --> tokens --> Unique ID --> vector embed\n",
    "tokenizer.decode(37534), tokenizer.decode(6197), tokenizer.decode(8335) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e5b8189d-daf7-4cf5-a2f5-b0741c4e1cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prep\n",
      "oster\n",
      "ous\n",
      ",\n",
      " prepare\n",
      " I\n",
      "'m\n",
      " fl\n",
      "ab\n",
      "berg\n",
      "asted\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "for token_id in input_ids[0]:\n",
    "    print(tokenizer.decode(token_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2ca5d429-cbe0-4942-a6e6-9a97ca137f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"I skipped across the\"\n",
    "input_ids = tokenizer(sentence, return_tensors='pt').input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9330e392-41f4-4f08-a7a2-2dde3eb982ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      " skipped\n",
      " across\n",
      " the\n"
     ]
    }
   ],
   "source": [
    "for token_id in input_ids[0]:\n",
    "    print(tokenizer.decode(token_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "76cc18d9-7e7a-4e45-8dde-4bbc8107f337",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2 = AutoModelForCausalLM.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "15e2ec04-7851-46da-b777-14b66a641c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = gpt2(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "65dea9cc-0116-4448-9838-4b18754ecbe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 50257])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ -39.3084,  -39.0100,  -41.8374,  ...,  -46.9337,  -44.9074,\n",
       "           -39.5149],\n",
       "         [ -95.4915,  -95.7003, -100.9737,  ..., -105.3830, -105.3877,\n",
       "           -97.9453],\n",
       "         [ -77.7412,  -78.2803,  -81.9967,  ...,  -85.0172,  -86.6475,\n",
       "           -80.5391],\n",
       "         [ -89.3046,  -87.9105,  -89.7545,  ...,  -90.2860,  -92.4808,\n",
       "           -88.3736]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(outputs.logits.shape)\n",
    "outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6563ac0c-105b-4429-8232-587e5ecbea5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-89.3046, -87.9105, -89.7545,  ..., -90.2860, -92.4808, -88.3736],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_logits = gpt2(input_ids).logits[0,-1]\n",
    "final_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "64420c43-0dcb-4647-9e38-f2f66e30a19a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4675)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_logits.argmax() # Token ID <--> Index location logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "199ec2a8-1823-4781-961b-ad552c5f2c6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' street'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(final_logits.argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1ac53631-53c9-4956-a9bd-bd74106bd55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "top10_logits = torch.topk(final_logits, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "df885e51-7cc6-48e5-bf6a-7d00b6e1824c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " street\n",
      " line\n",
      " room\n",
      " country\n",
      " river\n",
      " border\n",
      " pond\n",
      " bridge\n",
      " floor\n",
      " road\n"
     ]
    }
   ],
   "source": [
    "for index in top10_logits.indices:\n",
    "    print(tokenizer.decode(index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5a22eb98-b8f7-4a01-9061-d5934053558b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6.1705e-07, 2.4875e-06, 3.9347e-07,  ..., 2.3124e-07, 2.5757e-08,\n",
       "        1.5653e-06], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_logits.softmax(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "29d6dd86-8928-4c63-b21a-e12d805af8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "top10 = torch.topk(final_logits.softmax(dim=0), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1d032751-3a74-4f0c-a7f4-c0d833181f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " street -- 3.8%\n",
      " line -- 3.2%\n",
      " room -- 2.1%\n",
      " country -- 1.8%\n",
      " river -- 1.6%\n",
      " border -- 1.5%\n",
      " pond -- 1.5%\n",
      " bridge -- 1.3%\n",
      " floor -- 1.3%\n",
      " road -- 1.2%\n"
     ]
    }
   ],
   "source": [
    "for value, index in zip(top10.values, top10.indices):\n",
    "    print(f\"{tokenizer.decode(index)} -- {value.item():.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7b816d39-0def-4d9e-a590-5cc002269e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "output_ids = gpt2.generate(input_ids, max_new_tokens=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "98a4fea6-84a3-4f5f-b1d1-55d23d22a457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I skipped across the street to the store and bought a few more. I was surprised to find that the store was open\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(output_ids[0])\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ebfaa640-b202-4feb-a067-835a7b9b423c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I skipped across the street to a nearby restaurant, where I was greeted by an elderly man who asked me if he could\n"
     ]
    }
   ],
   "source": [
    "# Avoid repetition\n",
    "output_ids = gpt2.generate(input_ids, max_new_tokens=20, repetition_penalty=1.5)\n",
    "decoded_text = tokenizer.decode(output_ids[0])\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4419a62e-39b7-4211-bd4d-d405361d6779",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I skipped across the world two dozen times and had no idea it was an act of faith, but by some miracle I\n"
     ]
    }
   ],
   "source": [
    "# enable decoding strategies \n",
    "output_ids = gpt2.generate(input_ids, max_new_tokens=20, repetition_penalty=1.5, do_sample=True)\n",
    "decoded_text = tokenizer.decode(output_ids[0])\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "642c0aef-39bf-4f23-adb3-eed802b77d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I skipped across the country and found out that there was another place I could find it, in my own backyard. It\n"
     ]
    }
   ],
   "source": [
    "#  Top-K sampling\n",
    "output_ids = gpt2.generate(input_ids, max_new_tokens=20, repetition_penalty=1.5, do_sample=True, top_k=5)\n",
    "decoded_text = tokenizer.decode(output_ids[0])\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b27de1d6-9a20-4ddb-9784-fd480f688844",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I skipped across the room with his friend, who was sitting in a corner. He had never met him before and he\n"
     ]
    }
   ],
   "source": [
    "# Top-p sampling\n",
    "output_ids = gpt2.generate(input_ids, max_new_tokens=20, repetition_penalty=1.5, do_sample=True, top_k=5, top_p=0.94)\n",
    "decoded_text = tokenizer.decode(output_ids[0])\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "123774e0-2c9e-4bf0-8e72-8df37a893e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I skipped across the hallway, and then saw he could look to be about five miles farther. He got on her motorcycle\n"
     ]
    }
   ],
   "source": [
    "#  temperature argument to control the randomness\n",
    "output_ids = gpt2.generate(input_ids, max_new_tokens=20, repetition_penalty=1.5, do_sample=True, temperature=1.5)\n",
    "decoded_text = tokenizer.decode(output_ids[0])\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "eb980295-97b1-4f3b-b843-e6b750fdc5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I skipped across the street to a nearby store and bought some of my favorite things. I was so excited that it would\n"
     ]
    }
   ],
   "source": [
    " # temperature argument to control the randomness\n",
    "output_ids = gpt2.generate(input_ids, max_new_tokens=20, repetition_penalty=1.5, do_sample=True, temperature=0.1)\n",
    "decoded_text = tokenizer.decode(output_ids[0])\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdf4d55-9be9-42a5-9aec-3be0ed59d563",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"text-generation\", model=\"openai-community/gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5938ce78-ef35-41a6-8857-faa49d660a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"I went to the happy store today and bought a brand new Honda Civic. It arrived around 9 p.m. I'm not sure what was going on, though the Civic had the new name on it. Apparently the new name was the California Highway\"}]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"I went to the happy store today and bought a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4136cc87-7685-49f7-a8c0-6233b60fac61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
